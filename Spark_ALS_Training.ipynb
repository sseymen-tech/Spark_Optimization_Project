{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f46a18bf",
   "metadata": {},
   "source": [
    "## Spark and ALS Training and in a Recommender System Problem considering Provider Fairness, Diversity, and, Popularity Metrics\n",
    "\n",
    "In this notebook, we will recommend top-10 items to users using MovieLens-20M dataset. We have three metrics we work with and they are as follows in simple terms:\n",
    "\n",
    "Popularity: Penalize the more popular items, so that unpopular items will get a chance to get recommendations. Otherwise, usually recommender systems over-recommend the already popular items, creating a popularity bias cycle where popular items become more popular and unpopular items become more unpopular.\n",
    "\n",
    "Provider Fairness: Every provider will be recommended similar number of times. \n",
    "\n",
    "Diversity: Every user will be recommended movies with different genres.\n",
    "\n",
    "We start with training an ALS model and predicting every item-user pair ratings. Then, considering the predicted items, we keep the best of the best items offered from every supplier for every user, keeping the optimal solution intact while reducing the size of the data we need to keep significantly. This is a very important point, we remove items in such a way that we do not lose optimality, the removed items would never be offered in the first place considering the metrics of fairness, diversity, and popularity. Therefore, we save memory and speed significantly while not reducing the solution quality.\n",
    "\n",
    "In the last step, given the subset of items that are best of the best, we use a similar optimization model in \"A Unified Optimization Toolbox for Solving Popularity Bias, Fairness, and Diversity in Recommender Systems\" to solve our recommender system problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee6735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import col, explode\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext\n",
    "# sc.setCheckpointDir('checkpoint')\n",
    "spark2 = SparkSession.Builder().appName(\"Spark2\").\\\n",
    "    config(\"spark.executor.memory\", \"8g\").config(\"spark.driver.memory\",\"8g\").\\\n",
    "    getOrCreate()\n",
    "spark = spark2.newSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ef5c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "notebook_path = os.path.abspath(\"Spark_ALS_Training.ipynb\")\n",
    "file_loc = os.path.join(os.path.dirname(notebook_path), \"OneDrive/Desktop/Res.Fall22/ML20m/\")\n",
    "\n",
    "movies = spark.read.csv(file_loc+\"subratingsnew_includebadmovies.csv\",header=True)\n",
    "ratings = spark.read.csv(file_loc+\"ratingsnew_includebadmovies.csv\",header=True)\n",
    "\n",
    "ratings = ratings.\\\n",
    "    withColumn('userId2', col('userId2').cast('integer')).\\\n",
    "    withColumn('movieId2', col('movieId2').cast('integer')).\\\n",
    "    withColumn('rating', col('rating').cast('float'))\n",
    "    \n",
    "movies = movies.\\\n",
    "    withColumn('movieId2', col('movieId2').cast('integer')).\\\n",
    "    withColumn('rgenre', col('rgenre').cast('string')).\\\n",
    "    withColumn('lognorm', col('lognorm').cast('float')).\\\n",
    "    drop('movieId').drop('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2fe734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|userId2|movieId2|rating|\n",
      "+-------+--------+------+\n",
      "|      1|       2|   3.5|\n",
      "|      5|       2|   3.0|\n",
      "|     13|       2|   3.0|\n",
      "|     29|       2|   3.0|\n",
      "|     34|       2|   3.0|\n",
      "|     54|       2|   3.0|\n",
      "|     88|       2|   1.0|\n",
      "|     91|       2|   3.5|\n",
      "|    116|       2|   2.0|\n",
      "|    119|       2|   4.0|\n",
      "|    120|       2|   1.0|\n",
      "|    124|       2|   2.0|\n",
      "|    127|       2|   3.0|\n",
      "|    128|       2|   3.0|\n",
      "|    129|       2|   3.0|\n",
      "|    131|       2|   1.0|\n",
      "|    132|       2|   3.0|\n",
      "|    137|       2|   3.0|\n",
      "|    142|       2|   4.0|\n",
      "|    152|       2|   3.0|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------+----------+\n",
      "|movieId2|   rgenre|   lognorm|\n",
      "+--------+---------+----------+\n",
      "|       2|  Fantasy|0.88353753|\n",
      "|      29|Adventure| 0.7826069|\n",
      "|      32|   Sci-Fi| 0.9576034|\n",
      "|      47|  Mystery| 0.9534758|\n",
      "|      50| Thriller| 0.9622373|\n",
      "|     112|    Crime|  0.817661|\n",
      "|     151|  Romance|0.82389474|\n",
      "|     223|   Comedy| 0.8901887|\n",
      "|     253|   Horror| 0.9061119|\n",
      "|     260|Adventure| 0.9777998|\n",
      "|     293|    Crime| 0.8991568|\n",
      "|     296|    Crime|       1.0|\n",
      "|     318|    Drama| 0.9936492|\n",
      "|     337|    Drama|0.86057824|\n",
      "|     367|    Crime| 0.9293496|\n",
      "|     541|   Sci-Fi|0.91683203|\n",
      "|     589|   Action| 0.9733494|\n",
      "|     593|    Crime| 0.9935379|\n",
      "|     653|   Action| 0.8362756|\n",
      "|     919|  Musical| 0.8890731|\n",
      "+--------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()\n",
    "movies.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3da90",
   "metadata": {},
   "source": [
    "Ratings file:\n",
    "userId2: modified user_ids from MovieLens-20M dataset\n",
    "movieId2:  modified item_ids from MovieLens-20M dataset\n",
    "rating: rating of item-user pairs\n",
    "\n",
    "Movies file:\n",
    "movieId2:  modified item_ids from MovieLens-20M dataset\n",
    "rgenre:  If one movie has multiple genre tags, I selected one randomly among them.\n",
    "lognorm: A continuous value between [0, 1] which depicts the popularity of an item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f490d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0.6792096218051907\n"
     ]
    }
   ],
   "source": [
    "# Training ALS model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "## Can use grids to find the best model\n",
    "\n",
    "# Create test and train set\n",
    "(train, test) = ratings.randomSplit([0.9,0.1], seed = 1)\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId2\", itemCol=\"movieId2\", ratingCol=\"rating\", \n",
    "          nonnegative = True, implicitPrefs = False, coldStartStrategy=\"drop\",\n",
    "          maxIter =15, regParam=0.05, rank =100)\n",
    "\n",
    "# Import the requisite items\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "model = als.fit(ratings)\n",
    "# model.write().save('saved_model/1') \n",
    "# model = ALSModel.load('saved_model/1')\n",
    "\n",
    "# View the predictions\n",
    "test_predictions = model.transform(ratings)\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print('RMSE', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f5bd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, FloatType, IntegerType\n",
    "schema = StructType([ \\\n",
    "    StructField(\"userId2\",IntegerType(),True), \\\n",
    "    StructField(\"movieId2\",IntegerType(),True), \\\n",
    "    StructField(\"rating\",FloatType(),True), \\\n",
    "  ])\n",
    "    \n",
    "# Let us consider only a thousand user, distribute items to G many distinct groups. This means that every distinct group is \n",
    "# one provider and items are divided among them. Pop_penal value is the penalty term we want to apply to lognorm values.\n",
    "# Therefore, higher the lognorm value, higher the penalty applied to that item.\n",
    "\n",
    "itemno = 18340; userno = 138493; u_subset = 1000\n",
    "Group_no = 15; pop_penal = 1; topk = 10; genre_no=19\n",
    "u_subset = u_subset; i_subset = itemno ###small test data is created here\n",
    "\n",
    "## assign every item to a distinct provider group\n",
    "movies = movies.sort('movieId2')\n",
    "shuffled_groups = np.arange(1,1+Group_no).repeat(int(i_subset/Group_no))\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(shuffled_groups)\n",
    "\n",
    "schema_integer =StructType([ StructField(\"movieId2\",IntegerType(),True), \\\n",
    "                            StructField(\"group\",IntegerType(),True)  ])\n",
    "\n",
    "###ratings2 incorporated into the pop. penalized predictions and movie information\n",
    "cc = [(int(x+1), int(shuffled_groups[x])) for x in range(len(shuffled_groups))] \n",
    "df2 = spark.createDataFrame(data= cc, schema=schema_integer) \n",
    "movies = movies.join(df2,['movieId2'],'inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56256ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+-----------+----------+-----+-----------+\n",
      "|movieId2|userId2|rating|prediction|     rgenre|   lognorm|group|prediction2|\n",
      "+--------+-------+------+----------+-----------+----------+-----+-----------+\n",
      "|     148|     31|  0.11| 2.6675463|  Adventure|0.49997655|    5|  2.1675696|\n",
      "|     463|     31|  0.11| 2.2978752|      Drama| 0.4634931|    7|   1.834382|\n",
      "|     471|     31|  0.11| 2.7329488|     Comedy|0.81200963|   11|  1.9209392|\n",
      "|     496|     31|  0.11| 2.0440345|      Drama|0.46425992|    3|  1.5797746|\n",
      "|     833|     31|  0.11| 2.3764446|     Comedy| 0.5946691|   15|  1.7817755|\n",
      "|    1088|     31|  0.11| 2.4171207|    Musical| 0.8096021|    1|  1.6075187|\n",
      "|    1238|     31|  0.11| 2.5431688|     Comedy| 0.6794116|    5|  1.8637571|\n",
      "|    1342|     31|  0.11| 2.2901168|     Horror|0.68249434|   14|  1.6076224|\n",
      "|    1580|     31|  0.11|  4.000636|     Action| 0.9329459|    4|  3.0676901|\n",
      "|    1591|     31|  0.11| 3.6357775|     Sci-Fi| 0.7317807|    2|  2.9039967|\n",
      "|    1645|     31|  0.11| 3.1608827|    Mystery| 0.8137684|    3|  2.3471143|\n",
      "|    1829|     31|  0.11|  2.683638|    Romance|0.22873761|    4|  2.4549005|\n",
      "|    1959|     31|  0.11| 1.8803356|      Drama| 0.7268849|    4|  1.1534507|\n",
      "|    2122|     31|  0.11| 2.0939772|   Thriller|0.64926326|   12|   1.444714|\n",
      "|    2142|     31|  0.11| 3.0243614|    Musical| 0.6379767|   14|  2.3863847|\n",
      "|    2366|     31|  0.11| 2.5591156|     Horror| 0.7561793|   13|  1.8029363|\n",
      "|    2659|     31|  0.11| 2.0493255|Documentary|0.41558677|    6|  1.6337388|\n",
      "|    2866|     31|  0.11| 2.2165623|      Drama| 0.5931846|    6|  1.6233777|\n",
      "|    3175|     31|  0.11| 3.5452354|     Comedy| 0.8344289|   14|  2.7108064|\n",
      "|    3749|     31|  0.11| 2.3443534|      Drama|  0.691176|    7|  1.6531775|\n",
      "+--------+-------+------+----------+-----------+----------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "data2 = []    \n",
    "for userx in range(1,u_subset):\n",
    "    for i in range(1,i_subset+1): data2.append((userx,i,0.11)) ## Create data frame for user u for every item \n",
    "\n",
    "df = spark.createDataFrame(data=data2,schema=schema) \n",
    "test_predictions_for_this_user = model.transform(df) #Firstly we predict all the item-user pairs     \n",
    "\n",
    "ratings2 = test_predictions_for_this_user.join(movies,['movieId2'],'inner')\n",
    "ratings2 = ratings2.withColumn(\"prediction2\", F.col(\"prediction\") - F.col(\"lognorm\")*pop_penal) # penalize popularity\n",
    "\n",
    "ratings2.show() ## all the ratings for every user-item pair after the popularity penalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aab1e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "|movieId2|userId2|rating|prediction|     rgenre|    lognorm|group|prediction2|row|\n",
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "|    9684|      2|  0.11|  5.075377|    Romance|0.049434382|    2|   5.025943|  1|\n",
      "|   10393|      2|  0.11|  4.770469|     Sci-Fi|        0.0|    2|   4.770469|  2|\n",
      "|   14175|      2|  0.11|  4.754931|Documentary|        0.0|    2|   4.754931|  3|\n",
      "|   12048|      2|  0.11|  4.819127|  Animation| 0.10049947|    2|  4.7186275|  4|\n",
      "|   17137|      2|  0.11|  4.755494|  Animation| 0.09208068|    2|  4.6634135|  5|\n",
      "|    9317|      2|  0.11|  4.693764|      Drama| 0.09208068|    2|  4.6016836|  6|\n",
      "|   14195|      2|  0.11|  4.710457|Documentary| 0.16927862|    2|   4.541178|  7|\n",
      "|   11083|      2|  0.11| 4.5048018|      Drama|        0.0|    2|  4.5048018|  8|\n",
      "|   15316|      2|  0.11|  4.753965|  Animation| 0.25220758|    2|   4.501757|  9|\n",
      "|   16954|      2|  0.11| 4.4741116|    Western|        0.0|    2|  4.4741116| 10|\n",
      "|   11658|      3|  0.11| 5.0450616|Documentary| 0.03538972|    1|  5.0096717|  1|\n",
      "|   16619|      3|  0.11| 4.8303614|     Action| 0.08292894|    1|   4.747432|  2|\n",
      "|   10129|      3|  0.11|  4.733203|Documentary| 0.03538972|    1|   4.697813|  3|\n",
      "|   14385|      3|  0.11|  4.688092|Documentary|0.061822653|    1|  4.6262693|  4|\n",
      "|   16130|      3|  0.11|  4.634829|    Mystery|0.019176349|    1|  4.6156526|  5|\n",
      "|    8821|      3|  0.11| 4.6940346|      Drama| 0.11555064|    1|   4.578484|  6|\n",
      "|   10945|      3|  0.11|     4.573|      Crime|        0.0|    1|      4.573|  7|\n",
      "|   13267|      3|  0.11| 4.6182866|      Crime|0.049434382|    1|  4.5688524|  8|\n",
      "|    8896|      3|  0.11|  4.526542|    Mystery|        0.0|    1|   4.526542|  9|\n",
      "|   17283|      3|  0.11|  4.547737|     Comedy| 0.03538972|    1|   4.512347| 10|\n",
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowRate = Window.partitionBy('userId2',\"group\").orderBy(F.col(\"prediction2\").desc())\n",
    "topk_rates = ratings2.withColumn(\"row\",F.row_number().over(windowRate)) \\\n",
    "  .filter(F.col(\"row\") <= topk) \n",
    "\n",
    "topk_rates.show() ### Get top 10 items for user every u provider s according to penalized predictions ('prediction2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82b0ec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+---------+-----------+-----+-----------+---+\n",
      "|movieId2|userId2|rating|prediction|   rgenre|    lognorm|group|prediction2|row|\n",
      "+--------+-------+------+----------+---------+-----------+-----+-----------+---+\n",
      "|   15406|      1|  0.11| 4.0552235|  Musical|0.019176349|    2|   4.036047|  1|\n",
      "|   14939|      1|  0.11|  3.220876|Film-Noir| 0.07290433|    5|  3.1479716|  1|\n",
      "|    7945|      1|  0.11| 3.6485958|   Horror|        0.0|    5|  3.6485958|  1|\n",
      "|   13681|      1|  0.11| 3.4138377|Film-Noir| 0.03538972|   12|   3.378448|  1|\n",
      "|    8923|      1|  0.11| 3.8737524|Animation| 0.07290433|   15|   3.800848|  1|\n",
      "|   14167|      1|  0.11| 3.8388336| Thriller| 0.11555064|   15|  3.7232828|  1|\n",
      "|   11170|      2|  0.11|  4.338201|Animation| 0.08292894|    1|   4.255272|  1|\n",
      "|    6537|      2|  0.11| 3.9814568|     IMAX|  0.6525875|    1|  3.3288693|  1|\n",
      "|    9684|      2|  0.11|  5.075377|  Romance|0.049434382|    2|   5.025943|  1|\n",
      "|   10393|      2|  0.11|  4.770469|   Sci-Fi|        0.0|    2|   4.770469|  1|\n",
      "|   12704|      2|  0.11| 5.1435246|    Drama| 0.03538972|    3|  5.1081347|  1|\n",
      "|    8006|      2|  0.11| 4.1049685|  Fantasy| 0.10049947|    3|   4.004469|  1|\n",
      "|    1509|      2|  0.11| 4.3947425|Animation| 0.45354545|    5|   3.941197|  1|\n",
      "|    8984|      2|  0.11|  4.378405|    Crime|0.108294055|    5|   4.270111|  1|\n",
      "|    8101|      2|  0.11|  4.227428|  Romance| 0.09208068|    5|  4.1353474|  1|\n",
      "|   10541|      2|  0.11| 4.1382055|  Western|0.061822653|    6|  4.0763826|  1|\n",
      "|   13727|      2|  0.11| 3.8884895|     IMAX| 0.07290433|    7|  3.8155851|  1|\n",
      "|    6562|      2|  0.11| 3.7043843|   Horror|0.061822653|    9|  3.6425617|  1|\n",
      "|   15089|      2|  0.11|  3.723954|  Musical| 0.10049947|    9|  3.6234546|  1|\n",
      "|    3965|      2|  0.11| 3.9076684|Film-Noir| 0.39753705|   10|  3.5101314|  1|\n",
      "+--------+-------+------+----------+---------+-----------+-----+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowRate2 = Window.partitionBy('userId2',\"group\")\n",
    "genre_things2 = topk_rates.withColumn(\"cnt\" ,F.size(F.collect_set(\"rgenre\").over(windowRate2))).\\\n",
    "        drop('movieID2').drop(\"rating\").drop(\"prediction\").drop(\"rgenre\").drop(\"lognorm\").\\\n",
    "        drop(\"prediction2\").drop(\"row\").dropDuplicates()\n",
    "\n",
    "windowRate3 = Window.partitionBy('userId2',\"group\",\"rgenre\").orderBy(F.col(\"prediction2\").desc())\n",
    "top_genres = ratings2.withColumn(\"row\",F.row_number().over(windowRate3)) \\\n",
    "  .filter(F.col(\"row\") <= 1) \n",
    "\n",
    "top_genres.show() ## get the best item for each user and provider for each genre g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6b8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "|movieId2|userId2|rating|prediction|     rgenre|    lognorm|group|prediction2|row|\n",
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "|   11658|      1|  0.11|  4.792809|Documentary| 0.03538972|    1|   4.757419|  1|\n",
      "|   11819|      1|  0.11| 4.3358383|      Drama|        0.0|    1|  4.3358383|  2|\n",
      "|   10129|      1|  0.11| 4.3320255|Documentary| 0.03538972|    1|  4.2966356|  3|\n",
      "|   14613|      1|  0.11| 4.3614945|     Comedy| 0.07290433|    1|  4.2885904|  4|\n",
      "|   16619|      1|  0.11| 4.3326073|     Action| 0.08292894|    1|   4.249678|  5|\n",
      "|   14385|      1|  0.11|  4.279734|Documentary|0.061822653|    1|  4.2179117|  6|\n",
      "|    8896|      1|  0.11|  4.171053|    Mystery|        0.0|    1|   4.171053|  7|\n",
      "|   10945|      1|  0.11| 4.1559725|      Crime|        0.0|    1|  4.1559725|  8|\n",
      "|    1790|      1|  0.11| 4.2657275|    Western| 0.15094036|    1|   4.114787|  9|\n",
      "|   12203|      1|  0.11| 4.0732627|      Drama| 0.03538972|    1|   4.037873| 10|\n",
      "|   13641|      1|  0.11|  4.006092|    Romance|0.019176349|    1|  3.9869158|  8|\n",
      "|   10408|      1|  0.11|  3.845449|    Fantasy|0.019176349|    1|  3.8262727|  9|\n",
      "|   17420|      1|  0.11|  3.703664|  Adventure|        0.0|    1|   3.703664| 10|\n",
      "|   10393|      1|  0.11| 4.3899693|     Sci-Fi|        0.0|    2|  4.3899693|  1|\n",
      "|    9317|      1|  0.11| 4.3297544|      Drama| 0.09208068|    2|  4.2376738|  2|\n",
      "|   17667|      1|  0.11|  4.191397|     Action|        0.0|    2|   4.191397|  3|\n",
      "|   11083|      1|  0.11|  4.119524|      Drama|        0.0|    2|   4.119524|  4|\n",
      "|   15406|      1|  0.11| 4.0552235|    Musical|0.019176349|    2|   4.036047|  5|\n",
      "|    9684|      1|  0.11| 4.0843353|    Romance|0.049434382|    2|   4.034901|  6|\n",
      "|   14175|      1|  0.11| 4.0348325|Documentary|        0.0|    2|  4.0348325|  7|\n",
      "+--------+-------+------+----------+-----------+-----------+-----+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_genres_temp = top_genres.join(genre_things2,[\"userId2\",\"group\"])\n",
    "\n",
    "top_genres2 = top_genres_temp.withColumn(\"row\",F.row_number().over(windowRate)) \\\n",
    "  .filter((F.col(\"cnt\") < F.col(\"row\")) & (F.col(\"row\") <= topk)) \n",
    "\n",
    "ratings_last = topk_rates.unionByName(top_genres2.drop(\"cnt\")).\\\n",
    "    orderBy('userId2',\"group\",F.col(\"prediction2\").desc())\n",
    "\n",
    "ratings_last.show() ## Create a new list by items with best predicted ratings which also has at least 10 distinct genres. \n",
    "#Therefore, at this point we need all the items to find the optimal solution whether we care more about the predicted rating \n",
    "#or the diversity of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6bee9",
   "metadata": {},
   "source": [
    "At this point, rather than considering thousands of items for each user, we will consider around 15 items offered from every supplier (15) which is approximately 225 items. We already have a high quality subset of items, and can use heuristics to find a competitive solution. However, I would like to find the optimal solution by using optimization software Gurobi next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98390931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import *\n",
    "import pandas as pd\n",
    "\n",
    "df = ratings_last.toPandas() # with the rest of the calculations, I found it faster to work with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281d6b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-11-25\n",
      "Gurobi Optimizer version 10.0.0 build v10.0.0rc2 (win64)\n",
      "\n",
      "CPU model: 12th Gen Intel(R) Core(TM) i7-12700H, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 14 physical cores, 20 logical processors, using up to 20 threads\n",
      "\n",
      "Optimize a model with 21009 rows, 241904 columns and 929654 nonzeros\n",
      "Model fingerprint: 0x31e0bfa1\n",
      "Variable types: 0 continuous, 241904 integer (241904 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [2e+00, 7e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [8e+00, 8e+02]\n",
      "Found heuristic solution: objective 40057.639706\n",
      "Presolve removed 2138 rows and 2138 columns\n",
      "Presolve time: 0.87s\n",
      "Presolved: 18871 rows, 239766 columns, 925378 nonzeros\n",
      "Variable types: 0 continuous, 239766 integer (239766 binary)\n",
      "Found heuristic solution: objective 39496.699337\n",
      "Deterministic concurrent LP optimizer: primal simplex, dual simplex, and barrier\n",
      "Showing barrier log only...\n",
      "\n",
      "Root barrier log...\n",
      "\n",
      "Ordering time: 0.00s\n",
      "\n",
      "Barrier statistics:\n",
      " AA' NZ     : 1.993e+05\n",
      " Factor NZ  : 4.698e+05 (roughly 100 MB of memory)\n",
      " Factor Ops : 1.228e+07 (less than 1 second per iteration)\n",
      " Threads    : 12\n",
      "\n",
      "                  Objective                Residual\n",
      "Iter       Primal          Dual         Primal    Dual     Compl     Time\n",
      "   0   5.87409700e+06 -8.90564359e+05  1.26e+04 2.66e-01  2.70e+01     1s\n",
      "   1   1.19172873e+05 -6.90901811e+05  1.73e+02 4.44e-15  1.77e+00     1s\n",
      "   2   3.94166621e+04 -4.64249102e+04  6.71e-02 3.11e-15  1.73e-01     2s\n",
      "   3   3.89572666e+04  1.40292879e+04  5.55e-05 1.86e-15  5.01e-02     2s\n",
      "   4   3.85799816e+04  2.54436058e+04  3.44e-05 1.76e-15  2.64e-02     2s\n",
      "   5   3.81953275e+04  3.05137006e+04  2.44e-05 1.46e-15  1.54e-02     2s\n",
      "   6   3.77630982e+04  3.24911543e+04  1.75e-05 1.56e-15  1.06e-02     2s\n",
      "   7   3.72710808e+04  3.40655024e+04  1.08e-05 1.73e-15  6.44e-03     2s\n",
      "   8   3.67266704e+04  3.50721456e+04  3.76e-06 1.67e-15  3.33e-03     2s\n",
      "   9   3.65461190e+04  3.55548345e+04  2.04e-06 1.60e-15  1.99e-03     2s\n",
      "\n",
      "Barrier performed 9 iterations in 1.81 seconds (2.21 work units)\n",
      "Barrier solve interrupted - model solved by another algorithm\n",
      "\n",
      "Concurrent spin time: 0.04s\n",
      "\n",
      "Solved with dual simplex\n",
      "\n",
      "Root relaxation: objective 3.628264e+04, 17888 iterations, 0.81 seconds (0.72 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "*    0     0               0    36282.639022 36282.6390  0.00%     -    1s\n",
      "\n",
      "Explored 1 nodes (17888 simplex iterations) in 1.89 seconds (2.17 work units)\n",
      "Thread count was 20 (of 20 available processors)\n",
      "\n",
      "Solution count 3: 36282.6 39496.7 40057.6 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.628263902187e+04, best bound 3.628263902187e+04, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "mm = Model('Opt')\n",
    "\n",
    "u_list = list(set(df['userId2'].values)); i_list = list(set(df['movieId2'].values))\n",
    "U = len(u_list)\n",
    "\n",
    "genre_no_dict = {}; string_genre = list(set(df['rgenre'].values))\n",
    "R = len(string_genre)\n",
    "for j in range(R): genre_no_dict[j] = [j]\n",
    "\n",
    "dvar_tuple = tuple(df[['userId2','movieId2']].itertuples(index=False, name=None))\n",
    "x = mm.addVars(dvar_tuple, obj=df['prediction2'].values, vtype=GRB.BINARY, name =\"x\")\n",
    "y = mm.addVars(U,R, vtype=GRB.BINARY)\n",
    "\n",
    "####CONSTRAINTS\n",
    "mm.addConstrs(quicksum(x[(j,i)] for i in df[df['userId2'] == j]['movieId2'].values) == topk for j in u_list) ###Top-k list for every user\n",
    "\n",
    "w_constraint = 8 # recommend at least this amount of distinct items for every user\n",
    "mm.addConstrs(quicksum(y[j,r] for r in range(R)) >= w_constraint for j in range(U))\n",
    "for r in range(R):\n",
    "    r_string = string_genre[r]\n",
    "    temp = df[df['rgenre'] == str(r_string)]\n",
    "    mm.addConstrs(quicksum(x[(u_list[j],i)] for i in temp[temp['userId2'] == u_list[j]]['movieId2'].values) >= y[j,r] for j in range(U) ) \n",
    "\n",
    "## decide on lower bound and upper bound on number of recommendations of items from each supplier\n",
    "avg_rec = (U*topk)/Group_no ###number of total recommendations divided by the number of providers\n",
    "upper_alpha= 1.2\n",
    "lower_alpha= 0.8\n",
    "mm.addConstrs(quicksum(x[(j,i)]  for j in u_list for i in df[(df['group'] == g) & (df['userId2'] == j)]['movieId2'].values) <= np.ceil(upper_alpha*avg_rec) for g in range(1,Group_no+1))\n",
    "mm.addConstrs(quicksum(x[(j,i)]  for j in u_list for i in df[(df['group'] == g) & (df['userId2'] == j)]['movieId2'].values) >= np.floor(lower_alpha*avg_rec) for g in range(1,Group_no+1))\n",
    "#for bigger data, this part could be made faster by preprocessing the groups beforehand\n",
    "\n",
    "mm.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30e8eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_val = mm.getVars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e32d290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId2</th>\n",
       "      <th>userId2</th>\n",
       "      <th>rating</th>\n",
       "      <th>prediction</th>\n",
       "      <th>rgenre</th>\n",
       "      <th>lognorm</th>\n",
       "      <th>group</th>\n",
       "      <th>prediction2</th>\n",
       "      <th>row</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17420</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.703664</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.703664</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10933</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.646404</td>\n",
       "      <td>Animation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>3.646404</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>16192</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.694795</td>\n",
       "      <td>Western</td>\n",
       "      <td>0.049434</td>\n",
       "      <td>4</td>\n",
       "      <td>3.645361</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>7945</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.648596</td>\n",
       "      <td>Horror</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>3.648596</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>12959</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>3.755880</td>\n",
       "      <td>War</td>\n",
       "      <td>0.145809</td>\n",
       "      <td>5</td>\n",
       "      <td>3.610071</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movieId2  userId2  rating  prediction     rgenre   lognorm  group  \\\n",
       "12     17420        1    0.11    3.703664  Adventure  0.000000      1   \n",
       "52     10933        1    0.11    3.646404  Animation  0.000000      4   \n",
       "53     16192        1    0.11    3.694795    Western  0.049434      4   \n",
       "66      7945        1    0.11    3.648596     Horror  0.000000      5   \n",
       "67     12959        1    0.11    3.755880        War  0.145809      5   \n",
       "\n",
       "    prediction2  row  answer  \n",
       "12     3.703664   10     1.0  \n",
       "52     3.646404    9     1.0  \n",
       "53     3.645361   10     1.0  \n",
       "66     3.648596    9     1.0  \n",
       "67     3.610071   10     1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['answer'] = mm.x[0:len(x)]\n",
    "df2 = df[df['answer'] > 0.95]\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d16edca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Popularity 0.1524306163531136\n",
      "Diversity 8.20920920920921\n",
      "Fairness 0.9939569198332971\n",
      "Util 37805.42073702812\n"
     ]
    }
   ],
   "source": [
    "## Print metricsdf2['logmatch'] = df2['movieId2'].map(load_for_pop.set_index('movieId2')['lognorm'])    \n",
    "        \n",
    "Div = np.mean(df2.groupby('userId2')['rgenre'].nunique()) ## for each user check the number of distinct groups offered\n",
    "Group = df2.groupby('group').agg({'answer': 'count'}).values ## number of recommendations made from each provider\n",
    "Pop = sum(df2['lognorm'])/(U*10)  ## Total popularity value of the recommendation, smaller is better\n",
    "Util = sum(df2['prediction']) \n",
    "\n",
    "Fair = 0\n",
    "for group_fair in Group:\n",
    "    Fair -= (group_fair[0]/(U*10))*math.log(group_fair/(U*10),len(Group)) ##Z-metric for fairness, similar to entropy error.\n",
    "\n",
    "\n",
    "print('Popularity', Pop)\n",
    "print('Diversity', Div)\n",
    "print('Fairness',Fair)\n",
    "print('Util',Util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2aa8462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(lognorm)=0.31906942299628643)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "ratings2.select(avg('lognorm')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71954caa",
   "metadata": {},
   "source": [
    "We see that with popularity penalized and diversity incentivized we offer 8 distinct items in a top-10 list with much lower average popularity value than the average of the overall items provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27558971",
   "metadata": {},
   "source": [
    "Overall, the procedure is as follows:\n",
    "    \n",
    "   - Assume we know the items, groups, item popularity, item genre, and which provider group the items belong to.\n",
    "   - Train ALS and predict item-user utilities for all unknown values. \n",
    "   - Keep the best k many items offered for every user considering every provider. Note the number of different genres in this list as number n.\n",
    "   - Keep the best items offered from each genre for every user and provider.\n",
    "   - Combine the kept items for each user and provider noting that we only need k many distinct genres in the combined group.\n",
    "   - In this way, we obtain the most desirable items considering popularity, diversity, and predicted utility of the items.\n",
    "   - Time to use optimization software Gurobi to find the best solution for the recommender system problem. Our paper \"A Unified Optimization Toolbox for Solving Popularity Bias, Fairness, and Diversity in Recommender Systems\" explains the problem and its constraints in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970e25b7",
   "metadata": {},
   "source": [
    "Note that, we can have a lot of control over the metrics. We can decide how much diversity we want to include in our lists, or how much we want to penalize the popular items. Similarly, we can increase or decrease the constraints on the provider fairness according to the problem at hand. \n",
    "\n",
    "Scalability is an issue when we increase the number of users, however, this model can still solve up to tens of thousand of users at the same time. If we want to scale to millions, two options can be considered:\n",
    "-Large-scale optimization techniques such as Dantzig-Wolfe algorithm which I am currently working on.\n",
    "-Smart heuristics, such as, separating some of the users according to their preferences and solve smaller and more manageable chunks of optimization problems. This procedure will lead to suboptimal solutions, but applied correctly should not lead to significant objective function value drops. We discussed more on heuristics in our paper \"Making smart recommendations for perishable and stockout products.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_surprise] *",
   "language": "python",
   "name": "conda-env-python_surprise-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
